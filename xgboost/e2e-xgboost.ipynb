{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ade635",
   "metadata": {},
   "source": [
    "# End to End XGBoost\n",
    "\n",
    "https://github.com/mattharrison/talks\n",
    "\n",
    "©2023 MetaSnake\n",
    "\n",
    "`@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad077e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98049804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bba82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbc11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1d97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8cc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9d7ef6",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We will also use SHAP, xgbfir, openpyxl, hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "!pip install dtreeviz feature_engine pybaobabdt xgbfir shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7476ae6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import feature_engine\n",
    "from feature_engine import encoding, imputation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import base, compose, datasets, ensemble, \\\n",
    "    metrics, model_selection, pipeline, preprocessing, tree\n",
    "import xgboost as xgb\n",
    "import yellowbrick\n",
    "import yellowbrick.model_selection as ms\n",
    "from yellowbrick import classifier\n",
    "\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in [xgb, sklearn, yellowbrick, feature_engine]:\n",
    "    print(f'{str(mod)[9:20]} {mod.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c8700",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe04c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8916dc2",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "I'll be demoing with Kaggle 2018 survey data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca48a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "local = 'kaggle-survey-2018.zip'\n",
    "if not os.path.exists(local):\n",
    "    url = 'https://github.com/mattharrison/datasets/raw/master/data/kaggle-survey-2018.zip'\n",
    "    fin = urllib.request.urlopen(url)\n",
    "    data = fin.read()\n",
    "    with open(local, mode='wb') as fout:\n",
    "        fout.write(data)\n",
    "with zipfile.ZipFile(local) as z:\n",
    "    print(z.namelist())\n",
    "    kag = pd.read_csv(z.open('multipleChoiceResponses.csv'))\n",
    "    kag_questions = kag.iloc[0]\n",
    "    raw = kag.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdfbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn(ser, n=5, default='other'):\n",
    "    counts = ser.value_counts()\n",
    "    return ser.where(ser.isin(counts.index[:n]), default)\n",
    "\n",
    "def tweak_kag(df):\n",
    "    return (df\n",
    "        #.query('Q3.isin([\"United States of America\", \"China\", \"India\"]) '\\\n",
    "        #       'and Q6.isin([\"Data Scientist\", \"Software Engineer\"])')\n",
    "        .loc[df.Q3.isin([\"United States of America\", \"China\", \"India\"]) &\n",
    "             df.Q6.isin([\"Data Scientist\", \"Software Engineer\"])]\n",
    "        .pipe(lambda df_:\n",
    "            df_.assign(**(df_.Q1.pipe(pd.get_dummies, drop_first=True, prefix='gender')),\n",
    "                       age=df_.Q2.str.slice(0,2).astype(int),\n",
    "                       **(df_.Q3.pipe(pd.get_dummies, drop_first=True, prefix='country')),\n",
    "                       education=df_.Q4.replace({'Master’s degree': 18,\n",
    "                         'Bachelor’s degree': 16,\n",
    "                         'Doctoral degree': 20,\n",
    "                         'Some college/university study without earning a bachelor’s degree': 13,\n",
    "                         'Professional degree': 19,\n",
    "                         'I prefer not to answer': None,\n",
    "                         'No formal education past high school': 12}),\n",
    "                       **(df_.Q5\n",
    "                              .pipe(topn, n=3)\n",
    "                              .replace({\n",
    "                        'Computer science (software engineering, etc.)': 'cs',\n",
    "                        'Engineering (non-computer focused)': 'eng',\n",
    "                        'Mathematics or statistics': 'stat'})\n",
    "                              .pipe(pd.get_dummies, drop_first=True, prefix='major')),\n",
    "                       title=df_.Q6,\n",
    "                       years_exp=(df_.Q8.str.replace('+','', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .astype(float)),\n",
    "                       compensation=(df_.Q9.str.replace('+','', regex=False)\n",
    "                           .str.replace(',','', regex=False)\n",
    "                           .str.replace('500000', '500', regex=False)\n",
    "                           .str.replace('I do not wish to disclose my approximate yearly compensation', '0', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .fillna(0)\n",
    "                           .astype(int)\n",
    "                           .mul(1_000)\n",
    "                                    ),\n",
    "                       python=df_.Q16_Part_1.fillna(0).replace('Python', 1),\n",
    "                       r=df_.Q16_Part_2.fillna(0).replace('R', 1),\n",
    "                       sql=df_.Q16_Part_3.fillna(0).replace('SQL', 1)\n",
    "               )#assign\n",
    "              \n",
    "        )#pipe\n",
    "        .rename(columns=lambda col:col.replace(' ', '_'))\n",
    "        .loc[:, 'gender_Male':]   \n",
    "        .dropna()\n",
    "       )\n",
    "kag = tweak_kag(raw)\n",
    "kag_X = kag.drop(columns='title')\n",
    "kag_y = (kag.title == 'Data Scientist')\n",
    "kag_X_train, kag_X_test, kag_y_train, kag_y_test = model_selection.train_test_split(\n",
    "    kag_X, kag_y, stratify=kag_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dced3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecd8fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4e508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ceb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7343d7b6",
   "metadata": {},
   "source": [
    "## Stumps, Trees, and Forests\n",
    "\n",
    "Decision trees use a greedy algorithm to split on a feature (column) that results in the most \"pure\" split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3eca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True - DS\n",
    "kag_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(kag_X_train, kag_y_train)\n",
    "stump.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False - SE, Data Scientist - DS\n",
    "stump.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in kag_X_train.columns)\n",
    "_ = tree.plot_tree(stump, feature_names=features, filled=True, \n",
    "                   class_names=['SE', 'DS'], fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1602e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afbf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810f8f7a",
   "metadata": {},
   "source": [
    "## Underfit\n",
    "A stump is too simple. It has too much *bias*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Add more features\n",
    "* Use a more complex model\n",
    "\n",
    "For a tree we can let it grow deeper which should do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7e5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90b3b070",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "A model is too complicated. It has too much variance.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Simplify or constrain (*regularize*)\n",
    "* Add more samples\n",
    "\n",
    "For a tree we can prune back the growth so that the leaf nodes are overly specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c5f50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "hi_variance.fit(kag_X_train, kag_y_train)\n",
    "hi_variance.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b19961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has over 20 levels, limited to 5\n",
    "features = list(c for c in kag_X_train.columns)\n",
    "_ = tree.plot_tree(hi_variance, feature_names=features, filled=True, \n",
    "                   max_depth=5,\n",
    "                   class_names=['SE', 'DS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit view to first 2\n",
    "features = list(c for c in kag_X_train.columns)\n",
    "_ = tree.plot_tree(hi_variance, feature_names=features, filled=True, \n",
    "                   class_names=['SE', 'DS'], max_depth=2, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27eadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1427a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a1bff7",
   "metadata": {},
   "source": [
    "\n",
    "## Tree Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'max_depth=None' - Tree depth\n",
    "* 'max_features=None' - Amount of features to examine for split\n",
    "* 'max_leaf_nodes=None' - Number of leafs\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (*Impurity* : 0 - 100% accurate, .3 - 70%. Going from 70% to 100% accurate is a decrease of .3) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf.\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node.\n",
    "* 'min_weight_fraction_leaf=0' - The fraction fo the total weights required to be a leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9d6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c90149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54332643",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Uses *bagging* to ensemble many trees in an attempt to lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd7606",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "rf.fit(kag_X_train, kag_y_train)\n",
    "rf.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in kag_X_train.columns)\n",
    "_ = tree.plot_tree(rf.estimators_[0], feature_names=features, filled=True, \n",
    "                   max_depth=5,\n",
    "                   class_names=['SE', 'DS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1384c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd8b7b6",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'n_estimators=100' - Number of trees - should be *max_estimators*\n",
    "* 'oob_score=False' - Can estimate score when training (by using rows that weren't randomly selected). No need to hold out data\n",
    "* 'warm_start=False' - Can add more trees w/o starting over\n",
    "\n",
    "From tree:\n",
    "\n",
    "* 'max_depth=None' - Tree depth (1 to Infinity (`None`))\n",
    "* 'max_features=\"sqrt\"' - Amount of features to examine for split (1 to number of features (int). Float of percent (0. to 1.0). \"log2\" log2(n_features) or \"sqrt\"  sqrt(n_features). (Default square root number of features.)\n",
    "* 'max_leaf_nodes=None' - Number of leafs. Default (`None`) is unlimited.\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (0.0 to 1.0) (*Impurity* : 0 - 100% accurate, .3 - 70%) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf. (1 to n_samples).\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node. (1 to n_samples)\n",
    "* 'min_weight_fraction_leaf=0' - The fraction (0.0 to 1.0) of the total weights required to be a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0957d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how changing n_estimators affects score\n",
    "results = []\n",
    "rf_ws = ensemble.RandomForestClassifier(random_state=42, warm_start=True, n_estimators=1)\n",
    "rf_ws.fit(kag_X_train, kag_y_train)\n",
    "for i in range(2,100):\n",
    "    rf_ws.set_params(n_estimators=i)\n",
    "    rf_ws.fit(kag_X_train, kag_y_train)\n",
    "    # see other metrics\n",
    "    results.append(metrics.f1_score(kag_y_test, rf_ws.predict(kag_X_test)))\n",
    "pd.Series(results, index=range(2, 100)).plot(figsize=(8,4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how changing max_depth affects score\n",
    "results = []\n",
    "train_results = []\n",
    "vals = list(range(1,20))\n",
    "for i in vals:\n",
    "    rf_ws = ensemble.RandomForestClassifier(random_state=42, \n",
    "                                            max_depth=i)\n",
    "    rf_ws.fit(kag_X_train, kag_y_train)\n",
    "    results.append(metrics.f1_score(kag_y_test, rf_ws.predict(kag_X_test)))\n",
    "    train_results.append(metrics.f1_score(kag_y_train, rf_ws.predict(kag_X_train)))\n",
    "ax = pd.Series(results, index=vals, name='test').plot(figsize=(8,4))    \n",
    "pd.Series(train_results, index=vals, name='train').plot(ax=ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c93b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5e96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b3923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b3358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632eb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a68d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70248a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bcabce",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Uses *boosting* to train a series of (weak) trees that try to correct the error of the previous output. (For classification this is mapped to a probability)\n",
    "\n",
    "Like golfing (you continue to putt or use a different club depending on first error). Decision tree would be a single tee off. Random forest would be averaging the tee offs. \n",
    "\n",
    "* Regularization\n",
    "* Parallel Processing\n",
    "* Missing Number Support\n",
    "* Category Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d48468",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(kag_X_train, kag_y_train)\n",
    "xg.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f326db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's try w/ depth of 2 and 2 trees\n",
    "xg = xgb.XGBClassifier(max_depth=2, n_estimators=2)\n",
    "xg.fit(kag_X_train, kag_y_train)\n",
    "xg.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tree\n",
    "# leaf values are log probabilities (*logit*)\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=0, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second tree\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=1, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go down the left path with\n",
    "# this data\n",
    "row = pd.Series({'gender_Male': 0.0, 'gender_Prefer_not_to_say': 0.0, \n",
    "    'gender_Prefer_to_self-describe': 0.0, 'age': 30.0, 'country_India': 0.0, \n",
    "    'country_United_States_of_America': 1.0, 'education': 16.0, 'major_eng': 0.0, \n",
    "    'major_other': 0.0, 'major_stat': 0.0, 'years_exp': 0.0, 'compensation': 0.0, \n",
    "    'python': 0.0, 'r': 0.0, 'sql': 0.0}).to_frame().T\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba237a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result for DS = .4522\n",
    "# < .5 ... so Software Engineer!\n",
    "# this is [prob SE, prob DS]\n",
    "xg.predict_proba(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e89fb7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# sum up leafs and throw into \n",
    "# Example: no r, low ed, low exp\n",
    "# -.251 + 0.0602\n",
    "\n",
    "vals = np.linspace(-10, 10)\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "x = -.251 + 0.0602\n",
    "y = inv_logit(-.251 + 0.0602)\n",
    "print(f'({x:.2}, {y:.2})')\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(vals, inv_logit(vals))\n",
    "ax.plot([x], [y], marker='o')\n",
    "ax.set_xlim([-5, 5])\n",
    "_ = ax.set_xticks([-3, -2, -1, 0, 1, 2, 3])\n",
    "_ = ax.set_yticks([0,.4, .5, .6, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61928f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642596a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea10e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b51ec988",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Because you can keep \"putting\" you can keep track of how far away you are from the hole and stop when you are closest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712a136",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# defaults\n",
    "# 100 putts\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(kag_X_train, kag_y_train)\n",
    "xg.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# Go up to 100 but stop after you haven't improved for 20 hits\n",
    "# Min value at round 9\n",
    "\n",
    "xg = xgb.XGBClassifier(early_stopping_rounds=20)\n",
    "xg.fit(kag_X_train, kag_y_train,\n",
    "       eval_set=[(kag_X_train, kag_y_train),\n",
    "                 (kag_X_test, kag_y_test)], verbose=10)\n",
    "xg.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb60dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xg.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29449e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can get the evaluation metrics\n",
    "# validation_0 is for training data\n",
    "# validation_1 is for testing data\n",
    "results = xg.evals_result()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score is best at 11 trees\n",
    "results = xg.evals_result()\n",
    "ax = pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "              'testing': results['validation_1']['logloss'],\n",
    "             }).shift().plot(figsize=(5,4))\n",
    "ax.set_xlabel('ntrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114da264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66621b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a9579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e1e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dc85731",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* Boosting\n",
    "\n",
    "  * ``n_estimators=100`` - number of trees (or boosting rounds). Larger is more complex. Default 100. Use ``early_stopping_rounds`` with ``.fit`` to prevent overfitting.\n",
    "\n",
    "  * ``learning_rate=.3`` (called ``eta`` too) - after each boosting step, shrink feature weights. Larger is more conservative. Can be used with n_estimators to adjust time for convergence [0,1], default .3\n",
    "\n",
    "  * ``gamma=0`` / ``min_split_loss`` - L0 regularization. Global regularization. Minimum loss required for split. Larger is more conservative. [0, ∞], default 0 - No regularization.\n",
    "\n",
    "\n",
    "* Regularization\n",
    "\n",
    "  * ``reg_lambda=1`` - L2 regularization (Root of squared weights). Increase to be more conservative. Default 1\n",
    "  * ``reg_alpha=0`` - L1 regularization (Mean of weights). Increase to be more conservative. Default 0\n",
    "\n",
    "* Sampling - Use different rows\n",
    "\n",
    "  * ``subsample=1`` - Use % of samples (this is rows!) for next boosting round. Lower to more conservative. [0, 1], default 1. (When not equal to 1.0, model does *stochastic gradient descent*, ie. there is some randomness in the model.)\n",
    "\n",
    "\n",
    "New tree (sampling) parameters - Use different columns (not rows!):\n",
    "\n",
    "  * ``colsample_bytree=1`` - Fraction of columns for each boosting round.\n",
    "  \n",
    "  * ``colsample_bylevel=1`` - Fraction of columns for each depth level.\n",
    "  \n",
    "  * ``colsample_bynode=1`` - Fraction of columns for each node.\n",
    "  \n",
    "\n",
    "From tree:\n",
    "\n",
    "  * ``max_depth=6`` - depth of tree. Larger is more complex (more likely to overfit). How many feature interactions you can have. Each level doubles time. [0, ∞], default 6\n",
    "  * ``min_child_weight=1`` - Stop splitting after certain amount of purity. Larger will be more conservative.\n",
    "\n",
    "\n",
    "Imbalanced data:\n",
    "\n",
    "* ``scale_pos_weight=1`` -  ratio negative/positive. Default 1\n",
    "* Use ``'auc'`` or ``'aucpr'`` for ``eval_metric`` metric (rather than classification default ``'logless'``)\n",
    "* ``max_delta_step=0`` - try values from 1-10. Default 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dac9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try gamma on xgb\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X, kag_y,\n",
    "                    param_name='gamma', param_range=[0, .5, 1,2,5,10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a843de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X, kag_y,\n",
    "                    param_name='max_depth', param_range=[1,2,3,4,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fa77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this depends on n_estimators\n",
    "# should really use early stopping but yellowbrick doesn't support this 😢\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X, kag_y,\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f2342",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this takes a while to run (about 2 minutes)\n",
    "# can set scoring in GridSearchCV to \n",
    "# recall, precision, f1, accuracy\n",
    "params = {'reg_lambda': [0],  # No effect\n",
    "          'learning_rate': [.1, .3], # makes each boost more conservative (0 - no shrinkage) \n",
    "          #'colsample_bylevel': [.3, 1], # use 0, 50%, or 100% of columns in boost step\n",
    "          'subsample': [.7, 1],\n",
    "          #'gamma': [0, 1],\n",
    "          'max_depth': [1, 2, 3],\n",
    "          'random_state': [42],\n",
    "          'n_jobs': [-1],\n",
    "          #'early_stopping_rounds':[10],\n",
    "          'n_estimators': [200]}\n",
    "kag_xgb2 = xgb.XGBClassifier()\n",
    "cv = (model_selection.GridSearchCV(kag_xgb2, params, cv=3)#, n_jobs=-1)\n",
    "    .fit(kag_X_train, kag_y_train,\n",
    "         eval_set=[(kag_X_test, kag_y_test)],\n",
    "         early_stopping_rounds=5, verbose=10) \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f28ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe743ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# vs default\n",
    "params = {'learning_rate': 0.3,\n",
    " 'max_depth': 2,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 0.7}\n",
    "xgb_def2 = xgb.XGBClassifier()\n",
    "xgb_def2.fit(kag_X_train, kag_y_train)\n",
    "\n",
    "xgb_grid2 = xgb.XGBClassifier(**params)\n",
    "xgb_grid2.fit(kag_X_train, kag_y_train)\n",
    "xgb_def2.score(kag_X_test, kag_y_test), xgb_grid2.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8c1a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0dd032d",
   "metadata": {},
   "source": [
    "## Bonus: Tuning with Hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score  \n",
    "#https://bradleyboehmke.github.io/xgboost_databricks_tuning/index.html#slide21\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 12, 1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10),\n",
    "    'reg_alpha': hp.loguniform('alpha', -10, 10),\n",
    "    'reg_lambda': hp.loguniform('lambda', -10, 10),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 123,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093feee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(space):    \n",
    "    model = xgb.XGBClassifier(max_depth = int(space['max_depth']), \n",
    "                gamma = space['gamma'],                                         \n",
    "                reg_alpha = int(space['reg_alpha']),\n",
    "                min_child_weight=space['min_child_weight'],                                 \n",
    "                colsample_bytree=space['colsample_bytree'])\n",
    "    evaluation = [(kag_X_train, kag_y_train),\n",
    "            (kag_X_test, kag_y_test)]\n",
    "    model.fit(kag_X_train, kag_y_train,\n",
    "                 eval_set=evaluation, eval_metric=\"rmse\",            \n",
    "                 early_stopping_rounds=10,verbose=False)    \n",
    "         \n",
    "    pred = model.predict(kag_X_test)\n",
    "    accuracy = accuracy_score(kag_y_test, pred>0.5)    \n",
    "    print (\"SCORE:\", accuracy)    \n",
    "    #change the metric if you like    \n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ed13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,            \n",
    "    space=space,           \n",
    "    algo=tpe.suggest,            \n",
    "    max_evals=1000,            \n",
    "    trials=trials,\n",
    "    #timeout=60*5 # 5 minutes\n",
    "           )\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "best # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0eed04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.19514909424102928,\n",
    " 'colsample_bytree': 0.8227256149391048,\n",
    " 'gamma': 0.010701959121627006,\n",
    " 'lambda': 0.010955985134796302,\n",
    " 'learning_rate': 0.004570442245136879,\n",
    " 'max_depth': 3, \n",
    " 'min_child_weight': 0.2497193683952876,\n",
    " 'subsample': 0.6416201529297743}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(kag_X_train, kag_y_train),\n",
    "            (kag_X_test, kag_y_test)]\n",
    "xgb_hyp.fit(kag_X_train, kag_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(kag_X_test, kag_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8533664",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e329637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default and grid\n",
    "xgb_def2.score(kag_X_test, kag_y_test), xgb_grid2.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88145f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = xgb_grid2.get_params()\n",
    "hyp = xgb_hyp.get_params()\n",
    "for k in grid:\n",
    "    print(f'{k=:20} grid:{grid[k] or \"\":20} hyp:{hyp[k] or \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158e691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a851ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880ae74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ad963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a6e7fa1",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we've tuned our model, let's look at how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57341cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.19514909424102928,\n",
    " 'colsample_bytree': 0.8227256149391048,\n",
    " 'gamma': 0.010701959121627006,\n",
    " 'lambda': 0.010955985134796302,\n",
    " 'learning_rate': 0.004570442245136879,\n",
    " 'max_depth': 3, \n",
    " 'min_child_weight': 0.2497193683952876,\n",
    " 'subsample': 0.6416201529297743}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(kag_X_train, kag_y_train),\n",
    "            (kag_X_test, kag_y_test)]\n",
    "xgb_hyp.fit(kag_X_train, kag_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c0f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(kag_y_test, xgb_hyp.predict(kag_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ccc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(kag_y_test, xgb_hyp.predict(kag_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(kag_y_test, xgb_hyp.predict(kag_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.confusion_matrix(xgb_hyp, kag_X_train, kag_y_train,\n",
    "                            kag_X_test, kag_y_test,\n",
    "                            classes=['SE', 'DS']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_hyp,\n",
    "                       kag_X_test, kag_y_test,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff973a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.precision_recall_curve(xgb_hyp, kag_X_train, kag_y_train,\n",
    "                   kag_X_test, kag_y_test,\n",
    "                   classes=['SE', 'DS'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17f49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.classification_report(xgb_hyp, kag_X_train, kag_y_train,\n",
    "                   kag_X_test, kag_y_test,\n",
    "                   classes=['SE', 'DS'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504a4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726dc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36a251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acdf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631e114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda2ab73",
   "metadata": {},
   "source": [
    "## Training For Different Metrics\n",
    "\n",
    "We tuned our model. But we tuned it against accuracy. What if we want to optimize for recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy tuning\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X_train, kag_y_train,\n",
    "    #                param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab54f2-44a5-4c7e-8f78-4567908057ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision tuning - see scoring param\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X_train, kag_y_train,\n",
    "                    scoring='precision',\n",
    "                    #param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2049b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    kag_X_train, kag_y_train,\n",
    "                    scoring='f1',\n",
    "                    #param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cebea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c14cd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40e53c25",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a1be2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Trees are great when they overfit... They can explain what they overfit\n",
    "# (You can use these for \"surrogate models\")\n",
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "hi_variance.fit(kag_X_train, kag_y_train)\n",
    "hi_variance.score(kag_X_test, kag_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec28f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance shows the magnitude (not direction) of impact\n",
    "(pd.Series(hi_variance.feature_importances_, index=kag_X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost also supports feature importance\n",
    "xgb_def = xgb.XGBClassifier()\n",
    "xgb_def.fit(kag_X_train, kag_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacb33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(pd.Series(xgb_def.feature_importances_, index=kag_X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed226dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance is specific to model/hyperparameters\n",
    "(pd.Series(xgb_hyp.feature_importances_, index=kag_X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * \"weight\" is the number of times a feature appears in a tree\n",
    "# * \"gain\" is the average gain of splits which use the feature\n",
    "# * \"cover\" is the average coverage of splits which use the feature\n",
    "xgb.plot_importance(xgb_def, importance_type='cover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a840c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9e00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957fd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a69d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9917819",
   "metadata": {},
   "source": [
    "## Bonus: xgbfir (Feature Interactions Reshaped)\n",
    " *Gain*: Total gain of each feature or feature interaction\n",
    " \n",
    " *FScore*: Amount of possible splits taken on a feature or feature Interaction\n",
    " \n",
    " *wFScore*: Amount of possible splits taken on a feature or feature nteraction weighted by the probability of the splits to take place\n",
    " \n",
    " *Average wFScore*: wFScore divided by FScore\n",
    " \n",
    " *Average Gain*: Gain divided by FScore\n",
    " \n",
    " *Expected Gain*: Total gain of each feature or feature interaction weighted by the probability to gather the gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8b8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488f7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_def, feature_names=kag_X_train.columns, OutputXlsxFile='fir.xlsx')\n",
    "pd.read_excel('fir.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ec9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecbdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfcfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7319b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f27a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12765c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adba4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b2b86c5",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlantations)\n",
    "Should be *globally* consistent and accurate\n",
    "\n",
    " Shapley value (SHAP).\n",
    " \n",
    " From game theory, indicates how to distribute attribution of label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e8eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_def)\n",
    "vals = shap_ex(kag_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48959b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7f4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's explain an individual\n",
    "kag_X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75206d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_def.predict(kag_X_test.iloc[[0]])  # predicts SE... why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is also SE\n",
    "kag_y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base value. We sum up the scores.\n",
    "# > 0 Positive Case\n",
    "shap_ex.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < 0 therefore ... SE\n",
    "shap_ex.expected_value + vals.values[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue - SE\n",
    "# red - DS\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots.waterfall(vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2577a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'years_exp'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2326a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with jitter/alpha\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'years_exp'], ax=ax, x_jitter=.5, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70150b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with jitter/alpha\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'years_exp'], ax=ax, x_jitter=.5, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b895e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add interaction (color)\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'r'], color=vals, ax=ax, x_jitter=.5, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c43f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify interaction\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'years_exp'], color=vals[:, 'education'], ax=ax, x_jitter=.5, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418981ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(vals, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fc-list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb02681",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "XGBoost is very powerful. Combining with other tools will take you a long way.\n",
    "\n",
    "Explore your data and your results.\n",
    "\n",
    "Lots of libraries. Some are better integrated.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "* Pandas skills come in useful for manipulating data\n",
    "* Make sure you discuss business value with stake holders\n",
    "\n",
    "\n",
    "Questions?\n",
    "\n",
    "\n",
    "Connect on LinkedIn or Twitter `@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randrange(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randrange(1,5)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
